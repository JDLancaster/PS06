---
title: "STAT/MATH 495: Problem Set 06"
author: "Jeff Lancaster"
date: "2017-10-17"
output:
  html_document:
    toc: true
    toc_float: true
    toc_depth: 2
    collapsed: false
    smooth_scroll: false
    df_print: kable
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE, fig.width=8, fig.height=4.5, message=FALSE, warning = FALSE
  )
set.seed(76)

# Load packages
library(tidyverse)
library(broom)
library(knitr)
```


# Collaboration

Please indicate who you collaborated with on this assignment: 
Andrew Kim

# Setup

Define truth, which again we know for the purposes of this assignment, but in
practice we won't:

* the true function f(x) i.e. the signal
* the true epsilon i.e. the noise, which in this case is Normal$(0, sd=\sigma)$.
Hence the standard deviation $\sigma$ determines the amount of noise.

```{r}
f <- function(x) {
  x^2
}
sigma <- 0.3
```

This is the target point we'll be trying to predict: $(0.95, f(0.95)) = (0.95, 0.95^2) = (0.95, 0.9025)$, Thus, the test set is just `x=0.95`

```{r}
x0 <- 0.95
test_set <- data_frame(x=x0)
```

This function generates a random sample of size $n$; think of this as a "get new
data" function. Random in terms of both:

* (New) the predictor x (uniform on [0,1])
* the amount of noise $\epsilon$

```{r,echo=T}
generate_sample <- function(f, n, sigma) {
  sample <- data_frame(
    x = runif(n = n, min = 0, max = 1),
    f_x = f(x),
    epsilon = rnorm(n = n, mean = 0, sd = sigma),
    y = f_x + epsilon
  )
  # Recall: We don't observe f(x) and epsilon, just (x, y)
  sample <- sample %>% 
    select(x, y)
  return(sample)
}
```

Define

* The number $n$ of observations $(x_i, y_i)$ in each sample. In the handout,
$n=100$ to keep plots uncrowded. Here we boost to $n=500$
* Number of samples of size $n$ to consider

```{r}
n <- 500
n_sample <- 10000
```


# Computation

```{r}
set.seed(100)
datasetdf2 <- data.frame(matrix(ncol = 2, nrow = n_sample))
datasetdf2$X1<-.95
for (i in 1:n_sample){
  sampled_points <- generate_sample(f,500, sigma)
  test<-smooth.spline(x=sampled_points$x, y=sampled_points$y, df=2)
  datasetdf2$X2[i]<-predict(test,test_set)$y
}
df2Y<-0.95^2+rnorm(n_sample,mean=0,.3)
datasetdf2$'Y0'<-df2Y

datasetdf99 <- data.frame(matrix(ncol = 2, nrow = n_sample))
datasetdf99$X1<-.95
for (i in 1:n_sample){
  sampled_points <- generate_sample(f,500, sigma)
  test<-smooth.spline(x=sampled_points$x, y=sampled_points$y, df=99)
  datasetdf99$X2[i]<-predict(test,test_set)$y
}
df99Y<-0.95^2+rnorm(n_sample,mean=0,.3)
datasetdf99$'Y0'<-df99Y


MSEdf2 <- mean(((as.numeric(datasetdf2$X2)) - (datasetdf2$Y0))^2)
MSEdf99 <- mean(((as.numeric(datasetdf99$X2)) - (datasetdf99$Y0))^2)

biasSQdf2 <- mean(as.numeric(datasetdf2$X2) - 0.95^2)^2
biasSQdf99 <- mean(as.numeric(datasetdf99$X2) - 0.95^2)^2

varDF2 <- var(as.numeric(datasetdf2$X2))
varDF99 <- var(as.numeric(datasetdf99$X2))

sigma2 <- sigma^2
sigma99 <- sigma^2
```

```{r,echo=T}
table <- data.frame(matrix(ncol = 5, nrow = 2))
rownames(table)<-c("df2","df99")
colnames(table)<-c("MSE","Bias^2","Var","Irreducible","Sum")
table$MSE<-c(MSEdf2,MSEdf99)
table$'Bias^2'<-c(biasSQdf2,biasSQdf99)
table$Var<-c(varDF2,varDF99)
table$Irreducible<-c(sigma2,sigma99)
table$Sum<-c(biasSQdf2+varDF2+sigma2,biasSQdf99+varDF99+sigma99)

knitr::kable(table, digits = 4)
```

# Tables

As done in Lec 2.7, for both

* An `lm` regression AKA a `smooth.splines(x, y, df=2)` model fit 
* A `smooth.splines(x, y, df=99)` model fit 

output tables comparing:

|  MSE| bias_squared|   var| irreducible|   sum|
|----:|------------:|-----:|-----------:|-----:|
|     X|           X  |     X |      X |         X |

where `sum = bias_squared + var + irreducible`. You can created cleanly formatted tables like the one above by piping a data frame into `knitr::kable`.


# Analysis

**Questions**:

1. Based on the topics covered in Lec 2.7, name one possible "sanity check" for your results. Name another if you can.
1. In **two** sentences or less, give a rough sketch of what the procedure would
be to get the breakdown of $$\mbox{MSE}\left[\widehat{f}(x)\right]$$ for *all*
$x$ in this example, and not just for $$\mbox{MSE}\left[\widehat{f}(x_0)\right]
= \mbox{MSE}\left[\widehat{f}(0.95)\right]$$.
1. Which of the two models would you choose for predicting the point of interest and why?

**Answers**:

1. If our MSE isn't equal to the sum of our Bias^2+Var+Sigma^2, we are in trouble.  If these don't add up something is wrong.  Moreover, the variance for the df=2 should essentially be 0 and the bias for the df=99 should essentially be 0.
1. Instead of just evaluating for a point, coule we would evaluate over the whole interval by fitting a curve using multiple values of x (like we did for x=0.95 above).  Then, using the data from all of those new x values we calculated, we could find the expected difference between y and y_hat in an attempt to fit a function that models the underlying relationship between the two.
1. df=2 because the df99 value might be more of an outlier in general (extreme value of y hat) whereas if we just use df=2 we are guaranteed to get a value that is representative of the data as as whole, and therefore can't be that bad.  Really comes down to worst-case scenario here: df99 could be really bad, df2 will always be average.